{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "colab": {
      "name": "IDS575 - Quiz #03 - Logistic Regression and Binary Classification (PA)",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wi_wIVzzbJIU"
      },
      "source": [
        "# **IDS575: Machine Learning and Statistical Methods**\n",
        "## [Quiz #03 - Logistic Regression and Binary Classification]\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fs-KJZ7jbJIh"
      },
      "source": [
        "## Import Libraries\n",
        "* See various conventions and acronyms."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wAKMrSQubJIh"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dINsxbBvbJIi"
      },
      "source": [
        "## Load the data into a DataFrame\n",
        "* Read directly from a csv (excel-like) data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SxTBFShOc9xp",
        "outputId": "516ac9ab-c918-4995-e881-a893db32a7c7"
      },
      "source": [
        "FraudDataset = pd.read_csv('fraud.csv')\n",
        "print(type(FraudDataset))\n",
        "print(FraudDataset.keys())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Index(['Time', 'V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10',\n",
            "       'V11', 'V12', 'V13', 'V14', 'V15', 'V16', 'V17', 'V18', 'V19', 'V20',\n",
            "       'V21', 'V22', 'V23', 'V24', 'V25', 'V26', 'V27', 'V28', 'Amount',\n",
            "       'Class'],\n",
            "      dtype='object')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ooKj4Zwgtld"
      },
      "source": [
        "## Verify basic data statistics\n",
        "* Count the number of features. (i.e., attributes)\n",
        "* Count the number of examples. (i.e., instances and labels)\n",
        "* Unfortunately we don't know what each feature means due to privacy concerns.\n",
        "* Class variable: 0 (standard) / 1 (fradulent)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ycsrrcXeYx3",
        "outputId": "f49749c1-9d68-44c3-8b8b-af3eb592a05a"
      },
      "source": [
        "def printBasicStats(dataset):\n",
        "  print('- # of features = %d' % (len(dataset.keys()) - 1))\n",
        "  print('- # of examples = %d' % len(dataset))\n",
        "  \n",
        "printBasicStats(FraudDataset)\n",
        "print(FraudDataset)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "- # of features = 30\n",
            "- # of examples = 284807\n",
            "            Time         V1         V2        V3        V4  ...       V26  \\\n",
            "0            0.0  -1.359807  -0.072781  2.536347  1.378155  ... -0.189115   \n",
            "1            0.0   1.191857   0.266151  0.166480  0.448154  ...  0.125895   \n",
            "2            1.0  -1.358354  -1.340163  1.773209  0.379780  ... -0.139097   \n",
            "3            1.0  -0.966272  -0.185226  1.792993 -0.863291  ... -0.221929   \n",
            "4            2.0  -1.158233   0.877737  1.548718  0.403034  ...  0.502292   \n",
            "...          ...        ...        ...       ...       ...  ...       ...   \n",
            "284802  172786.0 -11.881118  10.071785 -9.834783 -2.066656  ...  0.250034   \n",
            "284803  172787.0  -0.732789  -0.055080  2.035030 -0.738589  ... -0.395255   \n",
            "284804  172788.0   1.919565  -0.301254 -3.249640 -0.557828  ... -0.087371   \n",
            "284805  172788.0  -0.240440   0.530483  0.702510  0.689799  ...  0.546668   \n",
            "284806  172792.0  -0.533413  -0.189733  0.703337 -0.506271  ... -0.818267   \n",
            "\n",
            "             V27       V28  Amount  Class  \n",
            "0       0.133558 -0.021053  149.62      0  \n",
            "1      -0.008983  0.014724    2.69      0  \n",
            "2      -0.055353 -0.059752  378.66      0  \n",
            "3       0.062723  0.061458  123.50      0  \n",
            "4       0.219422  0.215153   69.99      0  \n",
            "...          ...       ...     ...    ...  \n",
            "284802  0.943651  0.823731    0.77      0  \n",
            "284803  0.068472 -0.053527   24.79      0  \n",
            "284804  0.004455 -0.026561   67.88      0  \n",
            "284805  0.108821  0.104533   10.00      0  \n",
            "284806 -0.002415  0.013649  217.00      0  \n",
            "\n",
            "[284807 rows x 31 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O-a_Fm2du-mT"
      },
      "source": [
        "## Data inspection\n",
        "* See the label imbalance.\n",
        "* Measure the baseline accuracy.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ptx29gCt6mQo",
        "outputId": "ab0cc268-a9b4-450c-8fa8-c82593800673"
      },
      "source": [
        "Counts = FraudDataset['Class'].value_counts()\n",
        "print(Counts)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0    284315\n",
            "1       492\n",
            "Name: Class, dtype: int64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zaQuKmCXbJIj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "81034c42-e3ee-4a32-8b82-b695580480f3"
      },
      "source": [
        "pd.set_option('display.max_columns', 10)\n",
        "print(FraudDataset.describe(exclude=None))\n",
        "\n",
        "Counts = FraudDataset['Class'].value_counts()\n",
        "print(Counts)\n",
        "\n",
        "BaseLineAcc = Counts[0]/(Counts[0] + Counts[1])\n",
        "print(BaseLineAcc)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                Time            V1            V2            V3            V4  \\\n",
            "count  284807.000000  2.848070e+05  2.848070e+05  2.848070e+05  2.848070e+05   \n",
            "mean    94813.859575  1.758743e-12 -8.252298e-13 -9.636929e-13  8.316157e-13   \n",
            "std     47488.145955  1.958696e+00  1.651309e+00  1.516255e+00  1.415869e+00   \n",
            "min         0.000000 -5.640751e+01 -7.271573e+01 -4.832559e+01 -5.683171e+00   \n",
            "25%     54201.500000 -9.203734e-01 -5.985499e-01 -8.903648e-01 -8.486401e-01   \n",
            "50%     84692.000000  1.810880e-02  6.548556e-02  1.798463e-01 -1.984653e-02   \n",
            "75%    139320.500000  1.315642e+00  8.037239e-01  1.027196e+00  7.433413e-01   \n",
            "max    172792.000000  2.454930e+00  2.205773e+01  9.382558e+00  1.687534e+01   \n",
            "\n",
            "       ...           V26           V27           V28         Amount  \\\n",
            "count  ...  2.848070e+05  2.848070e+05  2.848070e+05  284807.000000   \n",
            "mean   ... -5.615260e-13  3.332112e-12 -3.518886e-12      88.349619   \n",
            "std    ...  4.822270e-01  4.036325e-01  3.300833e-01     250.120109   \n",
            "min    ... -2.604551e+00 -2.256568e+01 -1.543008e+01       0.000000   \n",
            "25%    ... -3.269839e-01 -7.083953e-02 -5.295979e-02       5.600000   \n",
            "50%    ... -5.213911e-02  1.342146e-03  1.124383e-02      22.000000   \n",
            "75%    ...  2.409522e-01  9.104512e-02  7.827995e-02      77.165000   \n",
            "max    ...  3.517346e+00  3.161220e+01  3.384781e+01   25691.160000   \n",
            "\n",
            "               Class  \n",
            "count  284807.000000  \n",
            "mean        0.001727  \n",
            "std         0.041527  \n",
            "min         0.000000  \n",
            "25%         0.000000  \n",
            "50%         0.000000  \n",
            "75%         0.000000  \n",
            "max         1.000000  \n",
            "\n",
            "[8 rows x 31 columns]\n",
            "0    284315\n",
            "1       492\n",
            "Name: Class, dtype: int64\n",
            "0.9982725143693799\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9noHAgdBbJIk"
      },
      "source": [
        "## Data inspection Part II.\n",
        "* Measure the correlation.\n",
        "* Let's draw heatmap as an intuitive visualization.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GIfrBGP-bJIk",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "322f5899-f33f-46da-f59e-34780282c1e9"
      },
      "source": [
        "print(FraudDataset.corr())\n",
        "\n",
        "import seaborn as sns\n",
        "sns.heatmap(FraudDataset.corr(), cmap=sns.diverging_palette(220, 10, as_cmap=True))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "            Time            V1            V2            V3            V4  ...  \\\n",
            "Time    1.000000  1.173963e-01 -1.059333e-02 -4.196182e-01 -1.052602e-01  ...   \n",
            "V1      0.117396  1.000000e+00  3.777386e-12 -2.119585e-12 -1.724017e-13  ...   \n",
            "V2     -0.010593  3.777386e-12  1.000000e+00  2.326821e-12 -2.314711e-12  ...   \n",
            "V3     -0.419618 -2.119585e-12  2.326821e-12  1.000000e+00  2.036710e-13  ...   \n",
            "V4     -0.105260 -1.724017e-13 -2.314711e-12  2.036710e-13  1.000000e+00  ...   \n",
            "V5      0.173072 -3.472870e-12 -1.832637e-12 -4.031527e-12 -2.549916e-13  ...   \n",
            "V6     -0.063016 -1.306592e-13  9.439633e-13 -1.582965e-13  1.083580e-12  ...   \n",
            "V7      0.084714 -1.127393e-13  5.403061e-12  3.404521e-12  8.130086e-13  ...   \n",
            "V8     -0.036949  2.115132e-12  2.153414e-14 -1.271766e-12  7.338233e-13  ...   \n",
            "V9     -0.008660  3.081639e-14  3.239499e-13 -6.819934e-13 -7.138427e-13  ...   \n",
            "V10     0.030617 -2.615209e-12  1.463139e-12 -1.611234e-12 -1.938801e-12  ...   \n",
            "V11    -0.247689  1.866050e-12 -8.312889e-13  8.717778e-13  1.875265e-12  ...   \n",
            "V12     0.124348 -1.238751e-12  6.139500e-13 -2.729282e-12  5.384794e-13  ...   \n",
            "V13    -0.065902  7.587057e-13 -1.180868e-12 -1.021796e-12  6.814508e-13  ...   \n",
            "V14    -0.098757 -1.870823e-13 -3.385766e-13 -5.568839e-13 -1.403949e-12  ...   \n",
            "V15    -0.183453 -3.599620e-13  2.200546e-13  6.438765e-13  1.526633e-12  ...   \n",
            "V16     0.011903 -1.142579e-12 -8.001072e-13 -8.741567e-13  3.078666e-13  ...   \n",
            "V17    -0.073297  1.671929e-12  2.028338e-12 -1.060595e-12  1.137838e-14  ...   \n",
            "V18     0.090438 -5.736991e-13 -1.911768e-14 -8.846121e-13 -1.309646e-12  ...   \n",
            "V19     0.028975 -2.770313e-12 -2.238263e-13 -1.060920e-12 -9.753449e-13  ...   \n",
            "V20    -0.050866  2.660170e-13  5.842566e-13  1.874043e-12 -2.347341e-12  ...   \n",
            "V21     0.044736 -3.276409e-12  2.280624e-12  6.735960e-13 -2.696261e-12  ...   \n",
            "V22     0.144059  2.281843e-12 -2.545709e-13 -8.922035e-13  4.343923e-13  ...   \n",
            "V23     0.051142 -2.971523e-12 -4.855967e-12  4.146836e-12 -4.161184e-12  ...   \n",
            "V24    -0.016182 -1.029987e-12  6.430858e-13  3.408174e-12 -2.368673e-12  ...   \n",
            "V25    -0.233083  1.146728e-12 -9.427890e-13  5.718362e-13  1.619552e-12  ...   \n",
            "V26    -0.041407  1.835367e-12 -4.128661e-13 -2.576520e-12 -3.045481e-13  ...   \n",
            "V27    -0.005135  7.624885e-12 -9.857578e-13 -5.041949e-12 -1.455904e-12  ...   \n",
            "V28    -0.009413 -9.774773e-13  2.525320e-12  5.188994e-12 -2.832333e-12  ...   \n",
            "Amount -0.010596 -2.277087e-01 -5.314089e-01 -2.108805e-01  9.873167e-02  ...   \n",
            "Class  -0.012323 -1.013473e-01  9.128865e-02 -1.929608e-01  1.334475e-01  ...   \n",
            "\n",
            "                 V26           V27           V28    Amount     Class  \n",
            "Time   -4.140710e-02 -5.134591e-03 -9.412688e-03 -0.010596 -0.012323  \n",
            "V1      1.835367e-12  7.624885e-12 -9.774773e-13 -0.227709 -0.101347  \n",
            "V2     -4.128661e-13 -9.857578e-13  2.525320e-12 -0.531409  0.091289  \n",
            "V3     -2.576520e-12 -5.041949e-12  5.188994e-12 -0.210880 -0.192961  \n",
            "V4     -3.045481e-13 -1.455904e-12 -2.832333e-12  0.098732  0.133447  \n",
            "V5     -1.895655e-13 -2.124508e-12  1.010195e-11 -0.386356 -0.094974  \n",
            "V6      3.351155e-12  1.480833e-12 -6.072099e-13  0.215981 -0.043643  \n",
            "V7     -4.475969e-12 -1.328689e-11  2.955517e-13  0.397311 -0.187257  \n",
            "V8      1.043929e-12 -3.500241e-12  1.866554e-12 -0.103079  0.019875  \n",
            "V9     -7.738355e-13  2.429466e-12 -1.406037e-12 -0.044246 -0.097733  \n",
            "V10    -2.739912e-13  1.552759e-12  5.116566e-12 -0.101502 -0.216883  \n",
            "V11     2.718052e-12 -3.950192e-12 -4.248014e-12  0.000104  0.154876  \n",
            "V12     2.816604e-13  5.952724e-13 -7.428139e-12 -0.009542 -0.260593  \n",
            "V13    -2.008516e-12  4.975675e-12 -6.778011e-12  0.005293 -0.004570  \n",
            "V14    -3.298712e-13 -2.447651e-12 -1.700001e-12  0.033751 -0.302544  \n",
            "V15     5.477463e-13 -4.690795e-12 -4.214974e-12 -0.002986 -0.004223  \n",
            "V16    -1.323907e-12  7.022680e-12  5.739623e-13 -0.003910 -0.196539  \n",
            "V17     2.941034e-12 -1.324300e-12  1.853972e-12  0.007309 -0.326481  \n",
            "V18    -1.810657e-12 -4.949657e-12  4.113090e-12  0.035650 -0.111485  \n",
            "V19     2.412142e-12 -2.201361e-12  3.450670e-12 -0.056151  0.034783  \n",
            "V20     1.466899e-13 -2.996593e-12  6.125392e-12  0.339403  0.020090  \n",
            "V21     8.462860e-13 -8.528788e-13  4.257426e-12  0.105999  0.040413  \n",
            "V22    -1.014966e-12 -1.726243e-13  5.948480e-12 -0.064801  0.000805  \n",
            "V23    -1.002527e-12  9.199453e-12  3.820045e-12 -0.112633 -0.002685  \n",
            "V24     1.604807e-12  1.554686e-12  1.380793e-11  0.005146 -0.007221  \n",
            "V25     2.112541e-12 -6.225186e-13 -8.597137e-12 -0.047837  0.003308  \n",
            "V26     1.000000e+00  2.374837e-12 -1.036867e-11 -0.003208  0.004455  \n",
            "V27     2.374837e-12  1.000000e+00 -4.440761e-12  0.028825  0.017580  \n",
            "V28    -1.036867e-11 -4.440761e-12  1.000000e+00  0.010258  0.009536  \n",
            "Amount -3.208037e-03  2.882546e-02  1.025822e-02  1.000000  0.005632  \n",
            "Class   4.455398e-03  1.757973e-02  9.536041e-03  0.005632  1.000000  \n",
            "\n",
            "[31 rows x 31 columns]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f577b0495f8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 58
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BY576FhOg7xa"
      },
      "source": [
        "## Data split\n",
        "* Must split into train and test data but with respect to the class distribution.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ljg5JPskg7Ma"
      },
      "source": [
        "from sklearn.model_selection import StratifiedShuffleSplit\n",
        "\n",
        "def splitTrainTest(df, size):\n",
        "  split = StratifiedShuffleSplit(n_splits=1, test_size=size, random_state=0)\n",
        "\n",
        "  # For each pair of train and test indices,\n",
        "  X = df.drop('Class', axis=1)\n",
        "  y = df.Class  \n",
        "  for trainIndexes, testIndexes in split.split(X, y):\n",
        "    X_train, y_train = X.iloc[trainIndexes], y.iloc[trainIndexes]\n",
        "    X_test, y_test = X.iloc[testIndexes], y.iloc[testIndexes]\n",
        "\n",
        "  return (X_train, y_train), (X_test, y_test)\n",
        "\n",
        "(X_train, y_train), (X_test, y_test) = splitTrainTest(FraudDataset, 0.2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TecDfoLCqFDi",
        "outputId": "24861d94-1d9d-4c7a-c715-9764052e30ad"
      },
      "source": [
        "print(y_train.value_counts())\n",
        "print(y_test.value_counts())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0    227451\n",
            "1       394\n",
            "Name: Class, dtype: int64\n",
            "0    56864\n",
            "1       98\n",
            "Name: Class, dtype: int64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WMcdoEH9-6Np"
      },
      "source": [
        "## Logistic Regression \n",
        "* Train the logistic regression.\n",
        "* Train again with normalization.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6At9tfyu_wcC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e3c61e38-ba68-4aaf-c787-ca449da7669a"
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix \n",
        "\n",
        "def doLogisticRegression(X, y, normalize=False):\n",
        "  # If normalize option is enabled,\n",
        "  if normalize:\n",
        "    # For each feature (indexed by j as usual)\n",
        "    for j in X.columns:\n",
        "      # Subtract its column mean and update the value.\n",
        "      X[j] -= X[j].mean()\n",
        "\n",
        "      # Divide by its standard deviation and update the value.\n",
        "      X[j] /= X[j].std()\n",
        "\n",
        "  # Instanciate an object from Logistic Regression class.\n",
        "  lr = LogisticRegression()\n",
        "\n",
        "  # Perform training and prediction.\n",
        "  lr.fit(X, y)\n",
        "  y_pred = lr.predict(X)\n",
        "      \n",
        "  # Return training accuracy and confusion matrix.\n",
        "  return accuracy_score(y, y_pred), confusion_matrix(y, y_pred), lr\n",
        "\n",
        "TrainAcc, TrainConf, LR = doLogisticRegression(X_train, y_train, normalize=True)\n",
        "print(TrainAcc)\n",
        "print(TrainConf)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.9992143781957032\n",
            "[[227418     33]\n",
            " [   146    248]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-5LcgQphsA6n",
        "outputId": "e2965f36-5c06-46d3-f03b-31095b7c180c"
      },
      "source": [
        "y_test_pred = LR.predict(X_test)\n",
        "TestAcc, TestConf = accuracy_score(y_test, y_test_pred), confusion_matrix(y_test, y_test_pred)\n",
        "print(TestAcc)\n",
        "print(TestConf)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.9979986657771848\n",
            "[[56848    16]\n",
            " [   98     0]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tmzeCBoik7-b"
      },
      "source": [
        "# Programming Assignment (PA)\n",
        "*   Implement logistic()\n",
        "*   Implement logLikelihood()\n",
        "*   Implement predict()\n",
        "*   Implement miniBatchGradientDescent()\n",
        "*   Play with testYourCode() that compares your implementations agaisnt scikit-learn's results.\n",
        "*   Note that your log-likelihood must increase over epoch as you update the model parameter theta toward its maximum."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QKm17sOc91qE"
      },
      "source": [
        "class MyLogisticRegression:\n",
        "  # Randomly initialize the parameter vector.\n",
        "  theta = None\n",
        "\n",
        "  def logistic(self, z):\n",
        "    # Return the sigmoid function value.\n",
        "    ##############################################################\n",
        "    # TO-DO: Complete the evaluation of logistic function given z.\n",
        "    logisticValue = \n",
        "    ##############################################################\n",
        "    return logisticValue\n",
        "\n",
        "  def logLikelihood(self, X, y):\n",
        "    # Compute the log-likelihood hood of all training examples.\n",
        "    # X: (m x (n+1)) data matrix\n",
        "    # y: (m x 1) output vector    \n",
        "\n",
        "    # If theta parameter has not trained yet,\n",
        "    if not isinstance(self.theta, np.ndarray):\n",
        "      return 0.0\n",
        "\n",
        "    # Compute the linear hypothesis given individual examples (as a whole).\n",
        "    h_theta = self.logistic(np.dot(X, self.theta))\n",
        "\n",
        "    # Evalaute the two terms in the log-likelihood.    \n",
        "    #################################################################\n",
        "    # TO-DO: Compute the two terms in the log-likelihood of the data.\n",
        "    probability1 = \n",
        "    probability0 = \n",
        "    #################################################################\n",
        "\n",
        "    # Return the average of the log-likelihood\n",
        "    m = X.shape[0]\n",
        "    return (1.0/m) * np.sum(probability1 + probability0) \n",
        "\n",
        "  def fit(self, X, y, alpha=0.01, epoch=50):\n",
        "    # Extract the data matrix and output vector as a numpy array from the data frame.\n",
        "    # Note that we append a column of 1 in the X for the intercept.\n",
        "    X = np.concatenate((np.array(X), np.ones((X.shape[0], 1), dtype=np.float64)), axis=1)\n",
        "    y = np.array(y)  \n",
        "\n",
        "    # Run mini-batch gradient descent.\n",
        "    self.miniBatchGradientDescent(X, y, alpha, epoch)\n",
        "\n",
        "  def predict(self, X):\n",
        "    # Extract the data matrix and output vector as a numpy array from the data frame.\n",
        "    # Note that we append a column of 1 in the X for the intercept.\n",
        "    X = np.concatenate((np.array(X), np.ones((X.shape[0], 1), dtype=np.float64)), axis=1)\n",
        "\n",
        "    # Perfrom a prediction only after a training happens.\n",
        "    if isinstance(self.theta, np.ndarray):\n",
        "      y_pred = self.logistic(X.dot(self.theta))\n",
        "      ####################################################################################\n",
        "      # TO-DO: Given the predicted probability value, decide your class prediction 1 or 0.\n",
        "      y_pred_class = \n",
        "      ####################################################################################\n",
        "      return y_pred_class\n",
        "    return None\n",
        "\n",
        "  def miniBatchGradientDescent(self, X, y, alpha, epoch, batch_size=100):    \n",
        "    (m, n) = X.shape\n",
        "  \n",
        "    # Randomly initialize our parameter vector. (DO NOT CHANGE THIS PART!)\n",
        "    # Note that n here indicates (n+1) because X is already appended by the intercept term.\n",
        "    np.random.seed(2) \n",
        "    self.theta = 0.1*(np.random.rand(n) - 0.5)\n",
        "    print('L2-norm of the initial theta = %.4f' % np.linalg.norm(self.theta, 2))\n",
        "    \n",
        "    # Start iterations\n",
        "    for iter in range(epoch):\n",
        "      # Print out the progress report for every 1000 iteration.\n",
        "      if (iter % 5) == 0:\n",
        "        print('+ currently at %d epoch...' % iter)   \n",
        "        print('  - log-likelihood = %.4f' % self.logLikelihood(X, y))\n",
        "\n",
        "      # Create a list of shuffled indexes for iterating training examples.     \n",
        "      indexes = np.arange(m)\n",
        "      np.random.shuffle(indexes)\n",
        "\n",
        "      # For each mini-batch,\n",
        "      for i in range(0, m - batch_size + 1, batch_size):\n",
        "        # Extract the current batch of indexes and corresponding data and outputs.\n",
        "        indexSlice = indexes[i:i+batch_size]        \n",
        "        X_batch = X[indexSlice, :]\n",
        "        y_batch = y[indexSlice]\n",
        "\n",
        "        # For each feature\n",
        "        for j in np.arange(n):\n",
        "          ####################################################################################\n",
        "          # TO-DO: Perform like a batch gradient desceint within the current mini-batch.\n",
        "          # Note that your algorithm must update self.theta[j].\n",
        "                   \n",
        "          ####################################################################################\n",
        "          \n",
        "  \n",
        "def doMyLogisticRegression(X, y, alpha, epoch, normalize=False):\n",
        "  # If normalize option is enabled,\n",
        "  if normalize:\n",
        "    # For each feature (indexed by j as usual)\n",
        "    for j in X.columns:\n",
        "      # Subtract its column mean and update the value.\n",
        "      X[j] -= X[j].mean()\n",
        "\n",
        "      # Divide by its standard deviation and update the value.\n",
        "      X[j] /= X[j].std()\n",
        "\n",
        "  # Instanciate an object from Logistic Regression class.\n",
        "  lr = MyLogisticRegression()\n",
        "\n",
        "  # Perform training and prediction.\n",
        "  lr.fit(X, y, alpha, epoch,)\n",
        "  y_pred = lr.predict(X)\n",
        "      \n",
        "  # Return training accuracy and confusion matrix.\n",
        "  return accuracy_score(y, y_pred), confusion_matrix(y, y_pred), lr\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1PrBUMlklvVE",
        "outputId": "17797dab-a53b-456c-ff69-bbb8a77ae89b"
      },
      "source": [
        "def testYourCode(X_train, y_train, X_test, y_test, alpha, epoch):\n",
        "  # Test the code with scikit-learn.\n",
        "  trainAcc, trainConf, lr = doLogisticRegression(X_train, y_train, normalize=True)\n",
        "  y_test_pred = lr.predict(X_test)\n",
        "  testAcc, testConf = accuracy_score(y_test, y_test_pred), confusion_matrix(y_test, y_test_pred)\n",
        "  print(\"Scikit's training/test accuracies = %.4f / %.4f\" % (trainAcc, testAcc))\n",
        "  print(\"Scikit's training/test confusion matrix\\n %s\\n %s\" % (trainConf, testConf))\n",
        "  theta = np.append(lr.coef_[0], lr.intercept_)\n",
        "  print(theta)\n",
        "\n",
        "  # Test the code with your own version.\n",
        "  myTrainAcc, myTrainConf, myLR = doMyLogisticRegression(X_train, y_train, alpha, epoch, normalize=True)\n",
        "  my_y_test_pred = myLR.predict(X_test)\n",
        "  myTestAcc, myTestConf = accuracy_score(y_test, my_y_test_pred), confusion_matrix(y_test, my_y_test_pred)\n",
        "  print(\"My training/test accuracies = %.4f / %.4f\" % (myTrainAcc, myTestAcc))\n",
        "  print(\"My training/test confusion matrix\\n %s\\n %s\" % (myTrainConf, myTestConf))\n",
        "  print(myLR.theta)\n",
        "\n",
        "testYourCode(X_train, y_train, X_test, y_test, 0.05, 100)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Scikit's training/test accuracies = 0.9992 / 0.9980\n",
            "Scikit's training/test confusion matrix\n",
            " [[227418     33]\n",
            " [   146    248]]\n",
            " [[56848    16]\n",
            " [   98     0]]\n",
            "[-1.66767445e-01  1.68357145e-01  2.62997389e-02  3.90900216e-02\n",
            "  9.42470155e-01  1.49724556e-01 -1.24410960e-01 -1.49150632e-01\n",
            " -2.04996491e-01 -2.87178956e-01 -8.38403194e-01 -5.15063282e-03\n",
            "  5.01854078e-02 -2.38173041e-01 -5.53811407e-01 -1.04186166e-01\n",
            " -2.22980387e-01  7.10037273e-03  7.00394204e-03  8.23866791e-02\n",
            " -3.54761664e-01  2.40133695e-01  4.12819490e-01 -5.76494056e-02\n",
            "  5.24238535e-02  2.44609365e-03 -2.80842684e-02 -2.87220019e-01\n",
            " -8.59162227e-02  2.58069755e-01 -8.63718232e+00]\n",
            "L2-norm of the initial theta = 0.1434\n",
            "+ currently at 0 epoch...\n",
            "  - log-likelihood = -0.6936\n",
            "+ currently at 5 epoch...\n",
            "  - log-likelihood = -0.0051\n",
            "+ currently at 10 epoch...\n",
            "  - log-likelihood = -0.0043\n",
            "+ currently at 15 epoch...\n",
            "  - log-likelihood = -0.0041\n",
            "+ currently at 20 epoch...\n",
            "  - log-likelihood = -0.0040\n",
            "+ currently at 25 epoch...\n",
            "  - log-likelihood = -0.0040\n",
            "+ currently at 30 epoch...\n",
            "  - log-likelihood = -0.0039\n",
            "+ currently at 35 epoch...\n",
            "  - log-likelihood = -0.0039\n",
            "+ currently at 40 epoch...\n",
            "  - log-likelihood = -0.0039\n",
            "+ currently at 45 epoch...\n",
            "  - log-likelihood = -0.0039\n",
            "+ currently at 50 epoch...\n",
            "  - log-likelihood = -0.0038\n",
            "+ currently at 55 epoch...\n",
            "  - log-likelihood = -0.0038\n",
            "+ currently at 60 epoch...\n",
            "  - log-likelihood = -0.0038\n",
            "+ currently at 65 epoch...\n",
            "  - log-likelihood = -0.0039\n",
            "+ currently at 70 epoch...\n",
            "  - log-likelihood = -0.0038\n",
            "+ currently at 75 epoch...\n",
            "  - log-likelihood = -0.0038\n",
            "+ currently at 80 epoch...\n",
            "  - log-likelihood = -0.0038\n",
            "+ currently at 85 epoch...\n",
            "  - log-likelihood = -0.0038\n",
            "+ currently at 90 epoch...\n",
            "  - log-likelihood = -0.0038\n",
            "+ currently at 95 epoch...\n",
            "  - log-likelihood = -0.0038\n",
            "My training/test accuracies = 0.9992 / 0.9978\n",
            "My training/test confusion matrix\n",
            " [[227415     36]\n",
            " [   142    252]]\n",
            " [[56836    28]\n",
            " [   98     0]]\n",
            "[-1.30514982e-01  1.56011330e-01  1.12112397e-01  5.55130445e-02\n",
            "  8.09899793e-01  1.87908037e-01 -1.42788493e-01 -1.93067608e-01\n",
            " -1.85468164e-01 -2.84448640e-01 -7.15219053e-01  2.29994833e-02\n",
            " -6.22665464e-03 -2.13489456e-01 -5.61397298e-01 -1.11100791e-01\n",
            " -2.29533522e-01  6.39001923e-03  1.61660288e-03  7.86409152e-02\n",
            " -3.46062280e-01  1.94598997e-01  3.66212143e-01 -3.09611013e-02\n",
            "  4.17779338e-02 -1.22148357e-02 -2.35247959e-02 -2.56901693e-01\n",
            " -7.87932304e-02  3.67708064e-01 -8.39003648e+00]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:7: RuntimeWarning: overflow encountered in exp\n",
            "  import sys\n"
          ],
          "name": "stderr"
        }
      ]
    }
  ]
}